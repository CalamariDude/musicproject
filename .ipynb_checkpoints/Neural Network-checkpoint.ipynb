{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 classes\n",
      "Shape of data is :  (200, 2)\n",
      "Shape of targets :  (200, 1)\n"
     ]
    }
   ],
   "source": [
    "import random as rand\n",
    "\n",
    "size_of_each_class = 100\n",
    "distributions = [[0,5],\n",
    "                [30, 15]]\n",
    "num_classes = len(distributions)\n",
    "print(num_classes, \"classes\")\n",
    "data = []\n",
    "targets = []\n",
    "dimensions = 2\n",
    "i = 0\n",
    "#For each distribution, generate guassian data and add to our data\n",
    "for mu, sigma in distributions:\n",
    "    #Generate size_of_each_class number of normal distibution datapoints\n",
    "    full_dimension = []\n",
    "    for j in range(dimensions):\n",
    "        gaussian_data = np.random.normal(mu, sigma, size_of_each_class)\n",
    "        full_dimension.append(gaussian_data)\n",
    "    full_dimension = np.asarray(full_dimension)\n",
    "    if(len(data) != 0) :\n",
    "        data = np.append(data, np.transpose(full_dimension), axis = 0)\n",
    "    else :\n",
    "        data = np.transpose(full_dimension)\n",
    "    \n",
    "    #Generate a target same size as each class size        \n",
    "    gen_targets = i * np.ones((1,size_of_each_class))        \n",
    "    targets.append(gen_targets)\n",
    "    i += 1\n",
    "\n",
    "targets = np.asarray(targets).flatten().reshape(200,1)\n",
    "\n",
    "print('Shape of data is : ', data.shape)\n",
    "print('Shape of targets : ', targets.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class nn:\n",
    "    def __init__(self, hiddenlayers = 2, neurons=5):\n",
    "        self.hiddenlayers = hiddenlayers\n",
    "        self.neurons = neurons\n",
    "    def activation(self, x):\n",
    "        result = 1/ (1 + np.exp(-x))\n",
    "        return result\n",
    "    def deactivate(self, x):\n",
    "        return self.activation(x) * (1-self.activation(x))\n",
    "    def cost(self, y, response):\n",
    "        return np.linalg.norm(y-response)**2\n",
    "    def train(self, x, y, epochs = 5, alpha=.1):\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "        w = []\n",
    "        if self.hiddenlayers == 0:\n",
    "            w.append(np.random.rand(x.shape[1], y.shape[1]))\n",
    "        else:\n",
    "            w.append(np.random.rand(x.shape[1], self.neurons))\n",
    "            for i in range(self.hiddenlayers-1):\n",
    "                w.append(np.random.rand(self.neurons, self.neurons))\n",
    "            w.append(np.random.rand(self.neurons, y.shape[1]))\n",
    "        print('Neural Network structure')\n",
    "        w = np.asarray(w)\n",
    "        for i in range(len(w)):\n",
    "            print(\"Layer\",i,\": \", w[i].shape)\n",
    "        \n",
    "        #Now its time to train\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for (datapoint, target) in zip(x, y):\n",
    "                xs = []\n",
    "                ys = []\n",
    "                xs.append(datapoint)\n",
    "                ys.append(datapoint)\n",
    "                \n",
    "                #Feed forward\n",
    "                for i in range(len(w)):\n",
    "                    weights = w[i]\n",
    "                    z = weights.T @ ys[-1]\n",
    "                    xs.append(z)\n",
    "                    ys.append(self.activation(z))\n",
    "                residvec = ys[-1] - target\n",
    "                residual = self.cost(ys[-1], target)\n",
    "                \n",
    "                loss += residual\n",
    "                ##Now its time to back propogate\n",
    "                gradients = []\n",
    "                lastgrad = self.deactivate(xs[-1]) * residvec\n",
    "                print(\"vars\", self.deactivate(xs[-1]).shape, residvec.shape)\n",
    "                gradients.append(lastgrad)\n",
    "                print(\"lastgrad shape\", lastgrad.shape)\n",
    "                for i in range(len(w)-1):\n",
    "                    print(\"vars\", self.deactivate(xs[i+1]).reshape((5,1)).shape, gradients[i].shape, w[len(w)-i-1].shape)\n",
    "                    nextgrad = self.deactivate(xs[i+1]).reshape((self.neurons,1)) * gradients[i]* w[len(w)-i-1]\n",
    "                    print(\"nextgrad shape\", nextgrad.shape)\n",
    "                    gradients.append(nextgrad)\n",
    "                print(\"Shape of gradients\")\n",
    "                for gradient in gradients:\n",
    "                    print(\"Gradient: \", gradient.shape)\n",
    "#                 print(\"Gradient layers = \", len(gradients))\n",
    "                #Update weights (from back to front)\n",
    "                for i in range(len(w)):\n",
    "                    print(\"Updating the \", len(w)-i-1, \"layer\")\n",
    "                    weights = w[len(w)-i-1]\n",
    "                    print(\"vars = \", gradients[i].shape, ys[len(ys)-i-2].shape, \"needs to be \", weights.shape)\n",
    "                    change = alpha * gradients[i] * ys[len(ys)-i-2] #y corersponds to the weights about to be multiplied\n",
    "                    print(\"shape of change\", change.shape)\n",
    "                    weights = np.subtract(weights, change)\n",
    "                    w[len(w)-i-1] = weights\n",
    "            print(\"loss = \" , loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network structure\n",
      "Layer 0 :  (2, 5)\n",
      "Layer 1 :  (5, 5)\n",
      "Layer 2 :  (5, 1)\n",
      "vars (1,) (1,)\n",
      "lastgrad shape (1,)\n",
      "vars (5, 1) (1,) (5, 1)\n",
      "nextgrad shape (5, 1)\n",
      "vars (5, 1) (5, 1) (5, 5)\n",
      "nextgrad shape (5, 5)\n",
      "Shape of gradients\n",
      "Gradient:  (1,)\n",
      "Gradient:  (5, 1)\n",
      "Gradient:  (5, 5)\n",
      "Updating the  2 layer\n",
      "vars =  (1,) (5,) needs to be  (5, 1)\n",
      "shape of change (5,)\n",
      "Updating the  1 layer\n",
      "vars =  (5, 1) (5,) needs to be  (5, 5)\n",
      "shape of change (5, 5)\n",
      "Updating the  0 layer\n",
      "vars =  (5, 5) (2,) needs to be  (2, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,5) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-59d7f48074ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(nn.predict(data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-cecd3ffb6ff6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, alpha)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vars = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"needs to be \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#y corersponds to the weights about to be multiplied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape of change\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,5) (2,) "
     ]
    }
   ],
   "source": [
    "nn = nn()\n",
    "nn.train(data, targets, epochs=30)\n",
    "# print(nn.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
